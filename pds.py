import math
import bitarray
import hashlib
import struct
class PDSFilter:

    def __init__(self, capacity=400000, error_rate=0.0001):
        if not (0 < error_rate < 1):
            raise ValueError("Error_Rate must be between 0 and 1.")
        if not capacity > 0:
            raise ValueError("Capacity must be > 0")
        
        number_of_hashes = int(math.ceil(math.log(1.0 / error_rate, 2)))
        size_of_bitmask = int(math.ceil(
            (capacity * abs(math.log(error_rate))) /
            ((math.log(2) ** 2))))
        self.bits_per_hash = size_of_bitmask // number_of_hashes
        self.K = number_of_hashes
        self.M = size_of_bitmask
        self.bitarray = bitarray.bitarray(self.M, endian='little')
        self.bitarray.setall(False)
        self.capacity = capacity
        self.count = 0
    
    def _get_sample_hashes(self, sample):
        num_of_hashes, remainder = divmod(self.K, hashlib.sha512().digest_size // 8)
        if remainder:
            num_of_hashes += 1
        hashes = (hashlib.sha512(struct.pack('I', i)) for i in range(num_of_hashes))

        num_of_keys = 0
        for hash in hashes:
            hash.update(sample['sha256'].encode('utf-8'))
            for key in struct.unpack("Q" * (hashlib.sha512().digest_size // 8), hash.digest()):
                yield key % self.bits_per_hash
                num_of_keys += 1
                if num_of_keys >= self.K:
                    return

    def __contains__(self, key):
        bits_per_slice = self.bits_per_hash
        bitarray = self.bitarray
        offset = 0
        for k in self._get_sample_hashes(key):
            if not bitarray[offset + k]:
                return False
            offset += bits_per_slice
        return True

    def __len__(self):
        """Return the number of keys stored by this bloom filter."""
        return self.count

    def add(self, ember_sample, skip_check=False):

        bitarray = self.bitarray
        bits_per_hash = self.bits_per_hash
        if self.count > self.capacity:
            raise IndexError("PDS is at capacity")
        offset = 0
        for k in self._get_sample_hashes(ember_sample):
            bitarray[offset + k] = True
            offset += bits_per_hash

        self.count += 1
   
    def tofile(self, f):
        """Write the bloom filter to file object `f'. Underlying bits
        are written as machine values. This is much more space
        efficient than pickling the object."""
        f.write(struct.pack("<QQQQQ", self.K,
                     self.M, self.capacity, self.count, self.bits_per_hash))
        f.write(self.bitarray.tobytes())

    @classmethod
    def fromfile(cls, f):
        """Read a bloom filter from file-object `f' serialized with
        ``BloomFilter.tofile''. If `n' > 0 read only so many bytes."""
        headerlen = struct.calcsize("<QQQQQ")

        
        K, M, capacity, count, bits_per_hash = struct.unpack("<QQQQQ", f.read(headerlen))
        filter = cls(1)  # Bogus instantiation, we will `_setup'.
        filter.K = K
        filter.M = M
        filter.capacity = capacity
        filter.count = count
        filter.bits_per_hash = bits_per_hash
        filter.bitarray = bitarray.bitarray(endian='little')
        filter.bitarray.frombytes(f.read())

        return filter

if __name__ == "__main__":
    import json
    from pathlib import Path
    import pandas as pd
    pds_filter = PDSFilter()
    pds_db = Path(r'pds.db')
    sha256_malicious = set()
    sha256_malicious_file = Path(r"C:\Users\Dekel\Downloads\task1_malicious_sha256_ember.txt")
    with sha256_malicious_file.open() as f:
        for sha256 in f.readlines():

            sha256_malicious.add(sha256[:-1].lower())

    ember_files = Path(r"C:\Users\Dekel\Downloads\ember_dataset_2018_2\ember2018").glob('*.jsonl')
    for ember_file in ember_files:
        with ember_file.open()  as f:
            for sample in f.readlines():
                sample = json.loads(sample)
                if sample['sha256'].lower() in sha256_malicious:
                    pds_filter.add(sample)
    ember_files = Path(r"C:\Users\Dekel\Downloads\ember_dataset_2018_2\ember2018").glob('*.jsonl')
    count_samples = 0
    for ember_file in ember_files:
        with ember_file.open()  as f:
            for sample in f.readlines():
                sample = json.loads(sample)
                if sample in  pds_filter:
                    count_samples += 1
    print(count_samples)
    assert count_samples >= 400000
    with pds_db.open("wb") as f:
        pds_filter.tofile(f)